{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac535cf-32e4-4c01-a6a2-ffc82f3828ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb67056-ac43-41fb-9c4a-ac5168ad822b",
   "metadata": {},
   "source": [
    "# YOLO \n",
    "\n",
    "En aquesta sessió descobrirem un dels models de l'estat de l'art (SOTA) de l'Aprenentatge Automàtic.\n",
    "\n",
    "You Only Look Once: Unified, Real-Time Object Detection. [article](https://arxiv.org/pdf/1506.02640) ; [presentació](https://www.youtube.com/watch?v=NM6lrxy0bxs&pp=ygUkeW91IG9ubHkgbG9vayBvbmNlIHByZXNlbnRhdGlvbiBjdnBy)\n",
    "\n",
    "YOLO (You Only Look Once) és una arquitectura de xarxa neuronal profunda inicialment dissenyada per a la detecció d'objectes en imatges en temps real. A diferència d'altres enfocaments que processen les imatges en diverses etapes com per exemple la família R-CNN. YOLO adopta un enfocament unificat: divideix la imatge en una quadrícula i processa cada cel·la simultàniament per predir les bounding boxes i les classes dels objectes presents. Aquesta integració permet assolir una velocitat notable sense comprometre significativament la precisió. A més, gràcies al seu disseny optimitzat, YOLO ha estat modificada i adaptada per realitzar múltiples tasques relacionades amb la visió per computador. Actualment pot: classificar, detectar, segmentar, seguiment d'objectes a vídeo, seguiment dels moviments del cos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499bd7aa-cae8-45ce-8d96-fb3163106f57",
   "metadata": {
    "tags": []
   },
   "source": [
    "### YOLO: Una breu història\n",
    "\n",
    "- **YOLO (You Only Look Once)**, un model popular de detecció d'objectes i segmentació d'imatges, va ser desenvolupat per Joseph Redmon i Ali Farhadi a la Universitat de Washington. Llançat el 2015, YOLO va guanyar ràpidament popularitat per la seva alta velocitat i precisió.\n",
    "- **YOLOv2**, llançat el 2016, va millorar el model original incorporant _batch normalization_ i  _anchor boxes_.\n",
    "- **YOLOv3**, llançat el 2018, va millorar encara més el rendiment del model mitjançant un _backbone_ més eficient, múltiples  _anchor boxes_ i agrupació de piràmides espacials (objectes de múltiples mides).\n",
    "- **YOLOv4** es va llançar el 2020, introduint innovacions com l'augment de dades emprant mosaics, un nou capçal (_head_) de detecció i una nova funció de pèrdua.\n",
    "- **YOLOv5** va millorar encara més el rendiment del model i va afegir noves funcions com ara l'optimització d'hiperparàmetres, el seguiment d'experiments integrat i l'exportació automàtica a formats d'exportació populars. [**Controvèrsia!!**](https://blog.roboflow.com/yolov4-versus-yolov5/)\n",
    "- **YOLOv6** va ser de codi obert per [Meituan](https://github.com/meituan/YOLOv6) el 2022 i s'utilitza en molts dels robots de lliurament autònoms de la companyia.\n",
    "- **YOLOv7** va afegir tasques addicionals, com ara l'estimació de poses (_pose estimation_) al conjunt de dades de punts clau COCO.\n",
    "- **YOLOv8** Es basa en l'èxit de les versions anteriors, introduint noves funcions i millores per millorar el rendiment, la flexibilitat i l'eficiència. YOLOv8 admet una gamma completa de tasques d'IA de visió, com ara detecció, segmentació, estimació de poses, seguiment i classificació. Aquesta versatilitat permet als usuaris aprofitar les capacitats de YOLOv8 en diferents aplicacions i dominis. --> Propaganda Ultralytics ^^.\n",
    "- **YOLOv9**, **YOLOv10**, **YOLOv11** : Successives millores en la xarxa, sobretot enfocades a mantenir les seves capacitats reduïnt la seva mida.\n",
    "\n",
    "Per començar i fer les primeres proves es recomana usar la versió 5, a que té un bon equilibri entre dificultat (complicacions d'ús) i els resultats que podem obtenir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f19f761-f55c-4010-b799-930925b8e90f",
   "metadata": {},
   "source": [
    "## Arquitectura\n",
    "\n",
    "La xarxa té 24 capes convolucionals seguides de 2 capes completament connectades. En lloc dels mòduls inicials utilitzats per [GoogLeNet](https://arxiv.org/pdf/1409.4842), per reduir el mapes d'activació s'utilitzen capes convolucionals 1×1 seguides de capes convolucionals de 3×3.\n",
    "\n",
    "![YOLO](img/YOLO.png \"YOLO\")\n",
    "\n",
    "\n",
    "### Detecció unificada\n",
    "\n",
    "A diferència de les xarxes que provenen de R-CNN a l'article s'explica:\n",
    "\n",
    "> Unifiquem els components separats de la detecció d'objectes en una única xarxa neuronal. La nostra xarxa utilitza funcions\n",
    "de tota la imatge per predir cada quadre delimitador. També prediu tots els quadres delimitadors (_bounding boxes_) de totes les classes per a una imatge simultàniament. Això vol dir que la nostra xarxa raona globalment sobre la imatge completa i tots els objectes de la imatge.\n",
    "\n",
    "El sistema YOLO divideix la imatge d'entrada en una graella $S×S$. Si el centre d'un objecte cau dins d'una cel·la de quadrícula, aquesta cel·la de quadrícula s'encarrega de detectar aquest objecte. Cada cel·la de la quadrícula prediu $B$ quadres de delimitació i els valors de confiança per a aquestes caixes. \n",
    "\n",
    "Aquestes puntuacions de confiança reflecteixen la confiança que té el model que la caixa conté un objecte i també la precisió que creu que és la caixa que prediu. Cada quadre delimitador consta de 5 prediccions: $x, y, w, h$ i la confiança. Les coordenades $(x, y)$ representen el centre del quadre en relació amb els límits de la cel·la de la quadrícula. L'amplada i l'alçada es prediuen en relació amb tota la imatge. Finalment, la predicció de confiança representa l'IOU entre la caixa predita i qualsevol caixa de veritat terrestre. Cada cel·la de la quadrícula també prediu probabilitats de classe condicional $C$.\n",
    "\n",
    "\n",
    "![YOLO](img/YOLO_deteccio.png \"YOLO\")\n",
    "\n",
    "L'arquitectura que hem mostrat anteriorment té una capa de sortida de $7x7x30$ degut a que  la sortida és correspon amb la següentv fórmula: $S × S × (B ∗ 5 + C)$ en el cas de l'article original: $S=7$, $B=2$ i $C=20$ ja que es va entrenar amb el dataset [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/).\n",
    "\n",
    "Actualment l'arquitectura de la xarxa és molt més complexa, i inclou 3 blocs molt diferenciats:\n",
    "\n",
    "- **Columna** \"Backbone\": Bàsicament és una xarxa convolucional que extreu característiques. A partir de la versió 3 creen la seva pròpia xarxa anomenada DarkNet, un model amb connexions residuals que té al voltant de 53 capes.\n",
    "- **Coll**: Aquesta part connecta la columna i el/els caps. S'encarrega entre d'altres coses de la detecció d'objectes a múltiples escales mitjançant xarxes piramidals que reben informació de diversos punts del \"Backbone.\n",
    "- **Cap**: El cap/caps s'encarrega de fer prediccions. En les versions modernes de YOLO s'utilitzen múltiples mòduls de detecció que prediuen quadres delimitadors, puntuacions d'objectivitat i probabilitats de classe per a cada cel·la de quadrícula del mapa de característiques. A continuació, aquestes prediccions s'agreguen per obtenir les deteccions finals.\n",
    "\n",
    "Podem veure un exemple d'aquesta complexa arquitectura en el següent enllaç a la documentació oficial de la [YOLO v5](https://docs.ultralytics.com/yolov5/tutorials/architecture_description/#1-model-structure)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e3b32c-07ee-4aa3-918d-99807c171dae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ús de la xarxa\n",
    "\n",
    "La manera més senzilla d'emprar la xarxa és desde la llibreria que la empresa Ultralytics ens ofereix. D'aquesta manera ens és molt senzill poder provar les diferents versions de la xarxa i també realitzar els processos de _fine tunning_ o de _transfer learning_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c442c9-a9a9-475f-84d4-b44d88c2480b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -U ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6337fa5-33c5-4ed8-92d2-248032570989",
   "metadata": {},
   "source": [
    "Començarem fent proves amb la YoloV5 que presenta 5 versions diferents, cada una d'aquestes versions té una xarxa _backbone_ de mida diferent, a més tenim 2 mides d'entrada d'imatges:\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Model</th>\n",
    "      <th>size<br><sup>(pixels)</sup></th>\n",
    "      <th>mAP<sup>val<br>50-95</sup></th>\n",
    "      <th>mAP<sup>val<br>50</sup></th>\n",
    "      <th>Speed<br><sup>CPU b1<br>(ms)</sup></th>\n",
    "      <th>Speed<br><sup>V100 b1<br>(ms)</sup></th>\n",
    "      <th>Speed<br><sup>V100 b32<br>(ms)</sup></th>\n",
    "      <th>params<br><sup>(M)</sup></th>\n",
    "      <th>FLOPs<br><sup>@640 (B)</sup></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n.pt\" target=\"_blank\">YOLOv5n</a></td>\n",
    "      <td>640</td>\n",
    "      <td>28.0</td>\n",
    "      <td>45.7</td>\n",
    "      <td><strong>45</strong></td>\n",
    "      <td><strong>6.3</strong></td>\n",
    "      <td><strong>0.6</strong></td>\n",
    "      <td><strong>1.9</strong></td>\n",
    "      <td><strong>4.5</strong></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt\" target=\"_blank\">YOLOv5s</a></td>\n",
    "      <td>640</td>\n",
    "      <td>37.4</td>\n",
    "      <td>56.8</td>\n",
    "      <td>98</td>\n",
    "      <td>6.4</td>\n",
    "      <td>0.9</td>\n",
    "      <td>7.2</td>\n",
    "      <td>16.5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m.pt\" target=\"_blank\">YOLOv5m</a></td>\n",
    "      <td>640</td>\n",
    "      <td>45.4</td>\n",
    "      <td>64.1</td>\n",
    "      <td>224</td>\n",
    "      <td>8.2</td>\n",
    "      <td>1.7</td>\n",
    "      <td>21.2</td>\n",
    "      <td>49.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l.pt\" target=\"_blank\">YOLOv5l</a></td>\n",
    "      <td>640</td>\n",
    "      <td>49.0</td>\n",
    "      <td>67.3</td>\n",
    "      <td>430</td>\n",
    "      <td>10.1</td>\n",
    "      <td>2.7</td>\n",
    "      <td>46.5</td>\n",
    "      <td>109.1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x.pt\" target=\"_blank\">YOLOv5x</a></td>\n",
    "      <td>640</td>\n",
    "      <td>50.7</td>\n",
    "      <td>68.9</td>\n",
    "      <td>766</td>\n",
    "      <td>12.1</td>\n",
    "      <td>4.8</td>\n",
    "      <td>86.7</td>\n",
    "      <td>205.7</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n6.pt\" target=\"_blank\">YOLOv5n6</a></td>\n",
    "      <td>1280</td>\n",
    "      <td>36.0</td>\n",
    "      <td>54.4</td>\n",
    "      <td>153</td>\n",
    "      <td>8.1</td>\n",
    "      <td>2.1</td>\n",
    "      <td>3.2</td>\n",
    "      <td>4.6</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s6.pt\" target=\"_blank\">YOLOv5s6</a></td>\n",
    "      <td>1280</td>\n",
    "      <td>44.8</td>\n",
    "      <td>63.7</td>\n",
    "      <td>385</td>\n",
    "      <td>8.2</td>\n",
    "      <td>3.6</td>\n",
    "      <td>12.6</td>\n",
    "      <td>16.8</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m6.pt\" target=\"_blank\">YOLOv5m6</a></td>\n",
    "      <td>1280</td>\n",
    "      <td>51.3</td>\n",
    "      <td>69.3</td>\n",
    "      <td>887</td>\n",
    "      <td>11.1</td>\n",
    "      <td>6.8</td>\n",
    "      <td>35.7</td>\n",
    "      <td>50.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l6.pt\" target=\"_blank\">YOLOv5l6</a></td>\n",
    "      <td>1280</td>\n",
    "      <td>53.7</td>\n",
    "      <td>71.3</td>\n",
    "      <td>1784</td>\n",
    "      <td>15.8</td>\n",
    "      <td>10.5</td>\n",
    "      <td>76.8</td>\n",
    "      <td>111.4</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x6.pt\" target=\"_blank\">YOLOv5x6</a><br>+ [TTA]</td>\n",
    "      <td>1280<br>1536</td>\n",
    "      <td>55.0<br><strong>55.8</strong></td>\n",
    "      <td>72.7<br><strong>72.7</strong></td>\n",
    "      <td>3136<br>-</td>\n",
    "      <td>26.2<br>-</td>\n",
    "      <td>19.4<br>-</td>\n",
    "      <td>140.7<br>-</td>\n",
    "      <td>209.8<br>-</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "Explicació mètrica [mAP](https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173)\n",
    "\n",
    "Nosaltres començarem fent proves amb la versió més petita:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0f76f9-1c2c-496c-97b8-b5bbebff4ac2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T07:04:42.442994Z",
     "start_time": "2024-11-25T07:04:33.126513Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a COCO-pretrained YOLOv5n model\n",
    "model = YOLO(\"yolov5n.pt\")\n",
    "\n",
    "# Display model information (optional)\n",
    "model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12111eb9-3ae8-4b54-b0bf-8cdd5aacd8df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980eac5e-e39f-4ebe-8389-d6869976fbfc",
   "metadata": {},
   "source": [
    "### Inferència\n",
    "\n",
    "YOLOv5 ha estat entrenat amb el dataset COCO (Common Objects in COntext) [enllaç](https://cocodataset.org/#home) que en té 80 classes diferents. Fer la inferència per detecció és molt senzill, és suficient amb cridar al model. Aquest ens retorna un objecte de tipus _Results_. [Documentació](https://docs.ultralytics.com/reference/engine/results/#ultralytics.engine.results.Results).\n",
    "\n",
    "Així, el procés d'inferència empra l'API d'Ultralytics i es fa enfora de _Pytorch_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac12e4c3-36ba-4812-af92-7c543bba6710",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Accepta URL, path, PIL, OpenCV, numpy o una llista\n",
    "img = \"https://hips.hearstapps.com/hmg-prod/images/the-boys-serie-amazon-1565605836.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16543479-afdc-41e8-888c-197a5233e2cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inferencia\n",
    "results = model(img)\n",
    "results; # És una llista de Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca54c459-c525-49a4-8d19-a6cbbf104081",
   "metadata": {},
   "source": [
    "#### Exercici\n",
    "\n",
    "Carrega una foto emprant la llibreria OpenCV (_cv2_) o la llibreria PIL, fes una predicció i mostra les caixes que envolten els objectes detectats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982f32c8-bce1-4cf8-8dc6-f50d33ad3421",
   "metadata": {},
   "source": [
    "### Entrenament\n",
    "\n",
    "Per realitzar l'entrenament s'empra el mètode `train` de la classe `YOLO`. No s'ha de realitzar cap bucle d'entrenament, sino que aquesta funció ens proporciona un nivell d'abstracció superior. És necessari especificar que aquest mètode és altament parametritzable i es fa necessari un estudi del mateix abans d'iniciar un entrenament.\n",
    "\n",
    "Consulta la documentació [enllaç](https://docs.ultralytics.com/modes/train/#key-features-of-train-mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8d7e8-92c5-4286-89ae-3dad9c53a45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Copiat de la documentació oficial\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a COCO-pretrained YOLO model\n",
    "model = YOLO(\"yolov5n.pt\") # també es pot carregar un model sense pre entrenar. Es troben en fitxers .yaml\n",
    "\n",
    "# Train the model on the COCO8 example dataset for 100 epochs\n",
    "results = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640) #NOTA: Aquí podem entrenar ja que coco8 \"es troba dins ultralytics\"\n",
    "\n",
    "# Run inference\n",
    "results = model(\"path/to/imatge.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c03c97-50b1-4ddf-8773-ad5106f91503",
   "metadata": {},
   "source": [
    "### Segmentació amb YOLO\n",
    "\n",
    "Encara que modificant la versió 5 es poden realitzar tasques de segmentació (veure [enllaç](https://github.com/ultralytics/yolov5/blob/master/segment/tutorial.ipynb)), és a partir de la versió 8 que aquesta tasca s'integra dins la xarxa amb l'incorporació d'un nou cap per aquesta tasca.\n",
    "\n",
    "A la documentació podem veure com ja tenim versions de tots els fitxers amb pesos per les diferents tasques: [enllaç](https://docs.ultralytics.com/models/yolov8/#supported-tasks-and-modes).\n",
    "\n",
    "A continuació veurem un exemple de segmentació:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e3c55c-49d1-497c-90e4-0351f9500bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a COCO-pretrained YOLOv8n model\n",
    "model = YOLO(\"yolov8n-seg.pt\")\n",
    "\n",
    "results_seg = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d2e79a-b4c6-4fdf-ac1a-dda8be38f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_result = results_seg[0].plot()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img_result);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa1ba96-7544-4071-9a2f-f55f7a79977f",
   "metadata": {},
   "source": [
    "#### Exercici\n",
    "\n",
    "En la imatge anterior tenim tant les capses de detecció com les segmentacions:\n",
    "1. Es demana que mostreu només la segmentació de la persona amb major valor de confiança de la xarxa.\n",
    "2. Es demana que mostreu per pantalla les coordenades de la capsa de detecció de la corbata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ee53c6-796f-4633-839e-c07a2711e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
