{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/bmalcover/aa_2425/blob/main/10_VGG_Custom_Dataset/VGG_Custom.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "</div>"
   ],
   "id": "e990e6801202c40d"
  },
  {
   "cell_type": "code",
   "id": "272d21b93244ea6",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import cv2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introducció\n",
    "\n",
    "En aquesta pràctica, treballarem amb un conjunt de dades d'imatges de cans i moixos que ja heu utilitzat a la pràctica de la part anterior de l'assignatura. Tot i que coneixem aquest conjunt de dades, en aquesta ocasió ho em d'emprar amb ``Pytorch`` per tant haurem d'adaptar com ho llegim: fent un pre-processat de les dades per aplicar-los nous models d'aprenentatge profund.\n",
    "\n",
    "L'objectiu de la sessió serà experimentar amb les diferents versions de la xarxa de convolució ``VGG``, com ara VGG16 i VGG19. Aquesta pràctica us permetrà veure com canvien els resultats d'entrenament i predicció en funció de l'arquitectura emprada, i reflexionar sobre els avantatges i inconvenients d'incrementar la profunditat d'una xarxa de convolució.\n",
    "\n",
    "# Preparam les dades\n",
    "Primerament descarregam les dades en el Google Colab. Per tal de fer-ho emprarem les eines ``wget`` i ``unzip``.\n"
   ],
   "id": "66a26d98415d24c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!wget https://github.com/bmalcover/aa_2425/releases/download/v1/gatigos.zip\n",
    "!unzip gatigos.zip"
   ],
   "id": "94c40c1e9bbd1603",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# *Custom dataset*\n",
    "\n",
    "Un dataset personalitzat en PyTorch permet preparar i gestionar dades específiques per a un projecte d’aprenentatge automàtic. La classe ``Dataset`` de ``PyTorch`` és la base per crear aquest tipus de dataset i requereix la implementació de tres mètodes essencials: ``__init__``, ``__len__``, i ``__getitem__``.\n",
    "\n",
    "* ``__init__``: Aquest mètode inicialitza el ``dataset`` i defineix els paràmetres que seran necessaris, com ara la ubicació de les dades o qualsevol transformació a aplicar. Aquí es poden carregar rutes d'imatges o etiquetes i definir les transformacions que es realitzaran.\n",
    "\n",
    "* ``__len__``: Aquest mètode retorna el nombre total d'exemples en el ``dataset``. ``PyTorch`` l'utilitza per saber quantes mostres conté el conjunt de dades, cosa que és essencial per crear les batchs d’entrenament.\n",
    "* ``__getitem__``: Aquest mètode accedeix a una mostra concreta del ``dataset``, identificada per un índex, i retorna les dades i la seva etiqueta (o ``target``). Normalment, s’apliquen les transformacions aquí abans de retornar la mostra, per assegurar que cada dada té el format adequat per al model.\n",
    "\n",
    "Un cop creada, aquesta classe es pot emprar amb el ``DataLoader`` de ``PyTorch`` per gestionar l'entrenament en *batchs*, fent que el dataset personalitzat sigui eficient i fàcil de treballar dins del flux d’aprenentatge automàtic de ``PyTorch``.\n"
   ],
   "id": "78dcf25d5b66ce14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class CatIGosDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, img_paths, annotations, transform=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            path: \n",
    "            path_anotation: \n",
    "            transform: \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.img_paths = img_paths\n",
    "        self.annotations = annotations\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.img_paths[idx])\n",
    "        label = self.annotations[self.img_paths[idx]]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label"
   ],
   "id": "c35b3d5385843893",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import glob\n",
    "\n",
    "paths_imgs = glob.glob(\"./catigos/images/*.png\")\n",
    "labels = {}\n",
    "\n",
    "for path_img in paths_imgs:\n",
    "    _, name = os.path.split(path_img)\n",
    "    name = name.split(\".\")[0]\n",
    "\n",
    "    name_xml = f\"./catigos/annotations/{name}.xml\"\n",
    "    tree = ET.parse(name_xml)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    annotation = root.find('object').find('name').text\n",
    "    annotation = 0 if annotation == 'cat' else 1\n",
    "\n",
    "    labels[path_img] = annotation"
   ],
   "id": "dd3915f6ecb80489",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train, test = train_test_split(paths_imgs, random_state=42, test_size=0.25)",
   "id": "8a26fdde567a454b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f040a135483ff08e",
   "metadata": {},
   "source": [
    "BATCH_SIZE = 4\n",
    "EPOCHS = 5\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_ds = CatIGosDataset(train, labels, transform=transform)\n",
    "test_ds = CatIGosDataset(test, labels, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b0756ea88b51da9e",
   "metadata": {},
   "source": [
    "img, target = next(iter(train_loader))\n",
    "print(img.shape, target)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fe645c3e13180bbe",
   "metadata": {},
   "source": [
    "# Definicio de la xarxa: VGG i *Transfer learning*\n",
    "\n",
    "En aquesta pràctica aplicarem la tècnica de transfer learning amb una de les xarxes CNN més conegudes i profundes:\n",
    "\n",
    " - VGG. [Very Deep Convolutional Networks for Large-Scale Image Recognition, 2014](https://arxiv.org/abs/1409.1556). La mida d'entrada de les imatges és de (224x224x3). VGG es presenta en diferents variants, com ara VGG16 i VGG19, que contenen respectivament 16 i 19 capes amb aproximadament 138 milions de paràmetres entrenables en el cas de VGG16.\n",
    "\n",
    "Descarregarem VGG i l'analitzarem. En aquest cas, no només obtenim la seva arquitectura, sinó també els pesos resultants del seu entrenament en grans conjunts de dades.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "55fbbcc900043cba",
   "metadata": {},
   "source": [
    "vgg11 = models.vgg11(weights=True)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Arquitectura VGG11\")\n",
    "print(\"-\" * 50)\n",
    "print(vgg11)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Com emprar la GPU per entrenar un model\n",
    "\n",
    "Un dels elements diferencials d'aquest model, respecte als que havíem vist fins ara, és la seva mida i, per tant, l'entrenament es torna impossible emprant __CPU__ directament. Per resoldre-ho hem d'emprar una **GPU**, a Google Colab disposam d'elles gratuïtament. Per fer-ho amb *Pytorch* hem de fer tres passes:\n",
    "\n",
    "1. Comprovar que hi ha una GPU disponible.\n",
    "2. Moure el model a GPU.\n",
    "3. Moure les dades a GPU.\n",
    "\n",
    "### Comprova si tenim una GPU disponible\n",
    "\n",
    "Primer de tot, cal verificar si hi ha una GPU disponible a l’entorn. Això es pot fer amb el següent codi:\n",
    "\n",
    "```python\n",
    "\n",
    "import torch\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "```\n",
    "\n",
    "Si la variable ``is_cuda`` és certa, llavors tens accés a una GPU.\n",
    "\n",
    "### Mou el model a la GPU\n",
    "\n",
    "En PyTorch, els models han d'estar explícitament en la GPU per poder fer servir la seva potència de càlcul. Si estàs carregant un model preentrenat (com AlexNet, ResNet, etc.), o si has definit el teu propi model, pots moure’l a la GPU amb ``.to(device)``, on device fa referència a la GPU.\n",
    "\n",
    "```python\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "```\n",
    "\n",
    "Això mou el model a la GPU (si està disponible). Si només tens una CPU, el model es mantindrà a la CPU.\n",
    "\n",
    "### Mou les dades a la GPU\n",
    "\n",
    "No només el model, sinó que també les dades (inputs) han d'estar a la GPU per fer les operacions més ràpides. Així, abans de fer servir les dades com a inputs del model, assegura't de moure-les al mateix device:\n",
    "\n",
    "```python\n",
    "\n",
    "# Exemple d'un batch de dades\n",
    "inputs, labels = inputs.to(device), labels.to(device)\n",
    "```\n"
   ],
   "id": "e1fbc1cbf5c836a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vgg11.to(device)"
   ],
   "id": "3cc3f9f3c6064fa8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5ee807ab8ee2cdf1",
   "metadata": {},
   "source": [
    "\n",
    "## Feina a fer:\n",
    " \n",
    "1. Preparar el *dataset* personalitzat. \n",
    "2. Carregar la xarxa VGG11, VGG16 i VGG19, amb i sense batch normalization.\n",
    "2. Entrenar-ho fent *transfer learning*.\n",
    "4. Comparar els resultats.\n",
    "\n",
    "**Nota**. Com veureu no us donam aquesta vegada el bucle d'entrenament, sigui com sigui podeu adaptar el vist a les sessions anteriors.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "\n",
    "vgg11.classifier = nn.Sequential(\n",
    "    torch.nn.Linear(9216, 1024),\n",
    "    nn.ReLU(),\n",
    "    torch.nn.Linear(1024, 1024),\n",
    "    nn.ReLU(),\n",
    "    torch.nn.Linear(1024, 512),\n",
    "    nn.ReLU(),\n",
    "    torch.nn.Linear(512, 1),  # Ja que tenim 2 classes.\n",
    ")"
   ],
   "id": "41cae526ac948721",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "learning_rate = 1e-3  # Hiperparàmetre\n",
    "optimizer = optim.Adam(vgg11.parameters(), lr=learning_rate)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = vgg11.to(device)"
   ],
   "id": "3fe4dddcb7238877",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "running_loss = []\n",
    "running_acc = []\n",
    "\n",
    "running_test_loss = []\n",
    "running_test_acc_cnn = []\n",
    "\n",
    "for t in tqdm(range(EPOCHS), desc=\"Èpoques\"):\n",
    "    batch_loss = 0\n",
    "    batch_acc = 0\n",
    "\n",
    "    i_batch = 1\n",
    "    # Iteram els batches.\n",
    "    for i_batch, (x, y) in tqdm(enumerate(train_loader), desc=f\"Batches (Època {t + 1})\"):\n",
    "        vgg11.train()  # Posam el model a mode entranament.\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 1. PREDICCIÓ\n",
    "        y_pred = vgg11(x.to(device))\n",
    "\n",
    "        # 2. CALCUL DE LA PÈRDUA\n",
    "        # Computa la pèrdua: l'error de predicció vs el valor correcte\n",
    "        # Es guarda la pèrdua en un array per futures visualitzacions\n",
    "\n",
    "        loss = loss_fn(y_pred, y.to(device))\n",
    "\n",
    "        #3. GRADIENT\n",
    "        vgg11.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Actualitza els pesos utilitzant l'algorisme d'actualització\n",
    "        #4. OPTIMITZACIO\n",
    "        with torch.no_grad():\n",
    "            optimizer.step()\n",
    "\n",
    "        # 5. EVALUAM EL MODEL\n",
    "        vgg11.eval()  # Mode avaluació de la xarxa\n",
    "\n",
    "        y_pred = vgg11(x.to(device)).detach().cpu().numpy()\n",
    "        batch_loss += (loss_fn(y_pred, y).detach())\n",
    "\n",
    "        y_pred_class = (y_pred > 0.5).double()\n",
    "        batch_acc += accuracy_score(y, y_pred_class)\n",
    "\n",
    "    running_loss.append(batch_loss / (i_batch + 1))\n",
    "    running_acc.append(batch_acc / (i_batch + 1))\n",
    "\n",
    "    batch_test_loss = 0\n",
    "    batch_test_acc = 0\n",
    "\n",
    "    vgg11.eval()\n",
    "    for i_batch, (x, y) in enumerate(test_loader):\n",
    "        y_pred = vgg11(x.to(device))\n",
    "        batch_test_loss += (loss_fn(y_pred, y.to(device)).detach())\n",
    "\n",
    "        y_pred_class = (y_pred > 0.5).double()\n",
    "        batch_test_acc += accuracy_score(y, y_pred_class)\n",
    "\n",
    "    running_test_loss.append(batch_test_loss / (i_batch + 1))\n",
    "    running_test_acc_cnn.append(batch_test_acc / (i_batch + 1))"
   ],
   "id": "47d511dc2607fac6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
