{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a35e40a-98ee-43cc-b084-b58c92f6b511",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SAM\n",
    "\n",
    "Segment Anything és un projecte de **Meta** amb dues grans aportacions:\n",
    "\n",
    "- Un gran conjunt de dades per a la segmentació d'imatges\n",
    "- El model Segment Anything (SAM) com a model de base per a la segmentació d'imatges\n",
    "\n",
    "Va ser introduït a l'article **Segment Anything** per Alexander Kirillov _et al._** (Abril 2023) [enllaç](https://arxiv.org/pdf/2304.02643.pdf)\n",
    "\n",
    "Aquest model s'inspira en el camp de la NLP (_Natural Language Processing_), on la creació de models base (**\n",
    "_foundation models_**) i els grans conjunts de dades (per valor de milers de milions de dades) s'han convertit en la manera habitual de fer feina.\n",
    "\n",
    "\n",
    "Com ja sabem, la segmentació d'imatges té diversos usos, aquests inclouen: l'anàlisi d'imatges biomèdiques, l'edició de fotografies i la conducció autònoma, entre d'altres. Per resoldre qualsevol d'aquests problemes, cal entrenar models especialitzats per cada una de les tasques que hem citat (és més, per cada un dels subproblemes que es deriven de cada tasca i per cada cas en particular). Això requereix un ampli coneixement del domini del problema i el temps necessari per a la recollida de dades específiques, per no parlar de les hores d'entrenament i ajustamen que són necessàries per als models d'aprenentatge profund.\n",
    "\n",
    "Enllaços:\n",
    "- [Repositòri Oficial](https://github.com/facebookresearch/segment-anything)\n",
    "- [Demo for dummies](https://segment-anything.com/demo)\n",
    "- [Ultralytics1](https://docs.ultralytics.com/models/sam/) i [Ultralytics2](https://docs.ultralytics.com/models/sam2/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610a8998ceb26253",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAM.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d141c08f-5c68-49de-a68e-a2c791dd34a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model\n",
    "\n",
    "SAM és un model d'aprenentatge profund (basat en _transformers_). I com amb qualsevol aprenentatge profund, s'ha entrenat en un gran nombre d'imatges i màscares: **més de mil milions de màscares en 11 milions d'imatges**.\n",
    "\n",
    "Com SAM és una model base, i està preparat per segmentar qualsevol tipus d'imatge, pot rebre indicacions dels usuaris sobre quina àrea segmentar. Actualment podem proporcionar tres indicacions diferents a SAM:\n",
    "\n",
    "- Indicar punts que contenen i no contenen el que volem segmentar.\n",
    "- Dibuixant un quadre delimitador (_bounding box_).\n",
    "- Dibuixant una màscara genearl sobre un objecte.\n",
    "\n",
    "En l'article és parla de preparar un _promt_ per poder especificar el que es vol segmentar emprant text.\n",
    "\n",
    "L'arquitectura de la xarxa és la següent:\n",
    "\n",
    "![SAM](https://learnopencv.com/wp-content/uploads/2023/04/segment-anything-model.png)\n",
    "\n",
    "\n",
    "Les caracterìstiques més importants del model (secció 3 de l'article) són:\n",
    "\n",
    "**Encoder**. Vision Transformer (ViT) basat en la idea d'un MAE (Masked AutoEncoder) El codificador d'imatge s'executa una vegada per imatge i es pot aplicar abans de sol·licitar el\n",
    "_prompt_ al model.\n",
    "\n",
    "**Codificador de _prompts_**. S'en consideren dos tipus: dispersos (punts, _bounding boxes_, text) i densos (màscares). Representen els punts i les _bounding boxes_ mitjançant codificacions posicionals amb altres informacions apreses per a cada tipus d'_input_ i text de forma lliure amb un codificador de text anomenat CLIP. Les indicacions denses (és a dir, les màscares) s'incorporen mitjançant l'ús convolucions i es sumen (concatenen) amb la codificació obtinguda de l'encoder de la imatge.\n",
    "\n",
    "**Descodificador de màscares**. El descodificador de màscara mapeja de manera eficient els _embeddings_ d'una imatge i el resultat del codificador de _prompts_ utilitzant una modificació d'un bloc descodificador Transformer \n",
    "\n",
    "**Resolució de l'ambigüitat**. Amb una sortida, el model farà una mitjana de diverses màscares vàlides si se li dóna una indicació ambigua. Per solucionar-ho, modifiquem el model per predir múltiples màscares de sortida per a un sol _prompt_ d'entreada. Experimentalment s'ha arribat a la conclusió que 3 màscares de sortida de màscara són suficients per abordar els casos més habituals (les màscares imbricades solen tenir tres profunditats com a màxim: senceres, parcials i subparts). \n",
    "\n",
    "**Eficiència**. El disseny global del model està motivat en gran mesura per l'eficiència. Donat un _embedding_ precalculat d'una imatge, el codificador de _promts_ i el descodificador de màscares s'executen en un navegador web, a la CPU, en ∼50 ms. Aquest rendiment en temps d'execució permet una indicació interactiva en temps real del model.\n",
    "\n",
    "**Pèrdues i entrenament.** Es supervisa la predicció de les màscares amb la combinació lineal d'una funció de pèrdua focal [enllaç](https://paperswithcode.com/method/focal-loss) i una funció de pèrdua de dice (ja emprada al model YOLO). L'entrenament es realitza utilitzant una barreja d'indicacions geomètriques, es simula un entrenament interactiu mitjançant un mostreig aleatori de prompts en 11 rondes per màscara.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fff5ea1-0000-4226-8d72-55c015bf5235",
   "metadata": {},
   "source": [
    "## Dades\n",
    "\n",
    "Com ja sabem, la base de qualsevol model d'aprenentatge profund innovador és el conjunt de dades en què s'ha entrenat. El conjunt de dades de SAM conté més d'**11 milions d'imatges i 1.100 milions de màscares**. El conjunt de dades final s'anomena conjunt de dades SA-1B.\n",
    "\n",
    "Segurament es necessita aquest conjunt de dades per entrenar un model de capacitat de Segment Anything. Però també sabem que aquests conjunts de dades no existeixen i que és impossible anotar manualment tantes imatges.Per tant es necessitar l'ajut de SAM per anotar el conjunt de dades: Els anotadors de dades van utilitzar SAM per anotar imatges de manera interactiva i les dades anotades es van utilitzar per entrenar SAM. Aquest procés es va repetir, cosa que va donar lloc al motor de dades en bucle de SAM.\n",
    "\n",
    "Aquest motor de dades + formació del SAM al conjunt de dades té tres etapes:\n",
    "\n",
    "- Etapa Manual Assistida\n",
    "- Etapa semiautomàtica\n",
    "- Etapa totalment automàtica\n",
    "\n",
    "En la primera etapa, els anotadors van utilitzar un model SAM prèviament entrenat per segmentar objectes de manera interactiva en imatges al navegador. Els _embeddings_ de les imatges es van calcular prèviament per fer que el procés d'anotació fos fluid i en temps real. Després de la primera etapa, el conjunt de dades constava de 4,3 milions de màscares a partir  de 120.000 imatges. El model Segment Anything es va tornar a entrenar en aquest conjunt de dades.\n",
    "\n",
    "En la segona etapa semiautomàtica, els objectes destacats ja estaven segmentats mitjançant SAM. Els anotadors també van anotar objectes menys destacats que no tenien anotació. Aquesta etapa va donar lloc a 5,9 milions de màscares addicionals en 180.000 imatges en les quals es va tornar a entrenar SAM.\n",
    "\n",
    "A l'etapa \"totalment automàtica\" final, l'anotació la va fer íntegrament SAM. En aquesta etapa, ja s'havia entrenat el model en més de 10 milions de màscares que ho van fer possible. La generació automàtica de màscares es va aplicar 11M a imatges , donant lloc a 1,1B  màscares.\n",
    "En paraules del seus creadors:\n",
    "\n",
    "\"La versió final del conjunt de dades Segment Anything el converteix en el conjunt de dades de segmentació d'imatges més gran disponible públicament. En comparació amb OpenImages V5, hi ha 6 vegades més imatges i 400 vegades més màscares al conjunt de dades.\"\n",
    "\n",
    "## SAM2\n",
    "\n",
    "SAM2 presenta una evolució de SAM, va ser publicat l'octubre de l'any 2024. [SAM 2: Segment Anything in Images and Videos](https://arxiv.org/pdf/2408.00714)\n",
    "\n",
    "SAM2 pot segmentar qualsevol objecte en qualsevol vídeo o imatge, fins i tot per a objectes i dominis visuals que no hagi vist anteriorment, permetent una àmplia gamma de casos d'ús sense adaptació personalitzada.\n",
    "\n",
    "Meta distibueix:\n",
    "- El codi i els pesos SAM 2, que s'estan de codi obert sota una llicència permissiva d'Apache 2.0.\n",
    "- El conjunt de dades SA-V, que té 4,5 vegades més vídeos i 53 vegades més anotacions que el conjunt de dades de segmentació de vídeo més gran existent. - Aquesta versió inclou ~ 51.000 vídeos del món real amb més de 600.000 mascares. \n",
    "- Una [demo](https://sam2.metademolab.com/) que permet la segmentació interactiva en temps real de vídeos curts i aplica efectes de vídeo a les prediccions del model.\n",
    "\n",
    "\n",
    "\n",
    "L'arquitectura de SAM2 s'ha adaptat per poder processar videos, però també s'ha treballat en reduïr la seva mida. En la [següent taula](https://docs.ultralytics.com/models/sam-2/#sam-2-comparison-vs-yolov8) en podem veure una comparativa molt interessant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f297ba-8cae-493d-a16b-ee8638826be6",
   "metadata": {},
   "source": [
    "## Tutorial\n",
    "\n",
    "Hi ha 3 models diferents preentrenats de SAM:\n",
    "\n",
    "- ViT-B SAM\n",
    "- ViT-L SAM\n",
    "- ViT-H SAM\n",
    "\n",
    "Per altra banda, tenim 4 models diferents de SAM2 (i de SAM2.1):\n",
    "\n",
    "- sam2_hiera_tiny\n",
    "- sam2_hiera_small\n",
    "- sam2_hiera_base_plus\n",
    "- sam2_hiera_large\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e088cc29-bd5c-428f-a8a7-37953601e863",
   "metadata": {},
   "source": [
    "Tal com passava amb YOLO es pot emprar SAM i SAM2 descarregant el model i/o els pesos desde els seus resperctius repositoris:\n",
    "\n",
    "- SAM [enllaç](https://github.com/facebookresearch/segment-anything#model-checkpoints).\n",
    "\n",
    "- SAM2 [enllaç](https://github.com/facebookresearch/sam2).\n",
    "\n",
    "Aquests models estàn implementats amb Pytorch i tenir la possibilitat d'accedir al codi font ens podría servir per tasques d'investigació.\n",
    "\n",
    "Per altra banda la empresa Ultralytics ha portat aquests dos models al seu entorn, amb la limitació que només podem fer predicció amb les condicions/restriccions de l'entron, però també aprofitant la seva simplesa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef5983c-61b6-4b0e-8bc9-cc4a958bf6b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Feina a fer\n",
    "0. Segmentar una única cel·lula a la imatge anomenada _cells.jpg_ que es troba a la carpeta _imgs_ mitjançant la selecció d'un punt i dibuixar la seva _bounding box_.\n",
    "1. Contar el nombre de cel·lules a la imatge anomenada _cells.jpg_ que es troba a la carpeta _imgs_.\n",
    "\n",
    "![celules](imgs/cells.jpg \"celules\")\n",
    "\n",
    "2. Segmentar un dels tres joves que apareixen a la imatge _str.jpg_, mostrar el resultat de la segmentació posant de color negre tot el que no sigui part de la segmentació. L'exercici és més senzill si emprau una bounding box.\n",
    "\n",
    "![str](imgs/str.jpg \"str\")\n",
    "\n",
    "3. Segmentar els tres joves emprant YOLO i una de les dues versions de SAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74dd337-13a1-49ca-9edd-ad79997a3965",
   "metadata": {},
   "source": [
    "A continuació teniu un exemple d'execució útil tant per SAM com per SAM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd9c5f0-6a7a-4c92-8dfd-50220c743f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.models.sam import Predictor as SAMPredictor\n",
    "from ultralytics.models.sam import SAM2VideoPredictor ## Per fer segmentació sobre vídeos\n",
    "\n",
    "# Diccionari amb la configuració del predictor\n",
    "overrides = dict(imgsz=1024, model=\"¿?\") # Si teniu moltes limitacions hard: mobile_sam.pt\n",
    "predictor = SAMPredictor(overrides=overrides)\n",
    "predictor.reset_image()\n",
    "# Assignam una imatge al predictor\n",
    "predictor.set_image(\"¿?\")  #pot ser un path o una url o un ndarray (numpy)\n",
    "\n",
    "\n",
    "# Exemple 1: Segmentació indicant un punt d'interés amb coordenades (x,y)\n",
    "#results = predictor(points=[x, y], labels=[1])  # label 1 indica objecte; label 0 indica fons\n",
    "\n",
    "# Exemple 2: Segmentació indicant dos punts d'interés amb coordenades (x1,y1); (x2,y2)\n",
    "#results = predictor(points=[[x1, y1], [x2, y2]], labels=[[1, 1]])\n",
    "\n",
    "# Exemple 3\n",
    "#results = predictor(points=[[[x1, y1], [x2, y2]]], labels=[[1, 0]])\n",
    "\n",
    "# Exemple 4: Segmentació indicant caixa\n",
    "#results = predictor(bboxes=[[[xi,yi], [xi, yf]]] )\n",
    "\n",
    "# Exemple 4: Segmentació indicant caixa\n",
    "#results = predictor(bboxes=[[[xi,yi], [xi, yf]]], points=[x,y], labels=[z] )\n",
    "# Reset image\n",
    "#predictor.reset_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97bdb85-1f09-4fd6-8d59-e308b9048cf0",
   "metadata": {},
   "source": [
    "També es pot fer una predicció de tota la imatge. No és recomanable ja que el temps d'inferéncia augmenta considerablement però pot ser d'utilitat en certs casos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cefa1c-9135-49c4-aa3f-379f1fcd2ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import SAM\n",
    "\n",
    "# Carregam un model\n",
    "model = SAM(\"mobile_sam.pt\")\n",
    "\n",
    "# Display model information (optional)\n",
    "model.info()\n",
    "\n",
    "# Executar l'inferència tal com ho feim a pytorch. Aquí podem passar la imatge directament.\n",
    "model(\"imgs/cells.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
