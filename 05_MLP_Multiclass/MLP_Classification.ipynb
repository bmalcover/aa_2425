{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc49bd18-5d7f-49b2-bcc2-a0d4df03d613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae4cebc-d802-428c-ad0f-3c96f9e79b2b",
   "metadata": {},
   "source": [
    "# Un problema real\n",
    "\n",
    "Fins ara, heu treballat amb problemes de *machine learning* molt simples, fent ús de dades tabulars, com ara fitxers amb taules de valors numèrics o categories. Això us ha permès aprendre les bases del procés de preparació de dades, la creació de models senzills i la seva avaluació.\n",
    "\n",
    "En aquesta pràctica, però, fareu un pas més enllà: per primer cop, treballareu amb dades en format d’imatges. Això suposarà un repte addicional, ja que les imatges són dades molt més complexes i necessiten tècniques especialitzades per ser tractades correctament. Aprendreu com carregar-les, quines són les millors pràctiques per treballar amb elles i com entrenar models de *deep learning* que siguin capaços de reconèixer patrons visuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548a267d-a7bb-4170-8405-24aadaabe887",
   "metadata": {},
   "source": [
    "## Lectura de dades\n",
    "\n",
    "La primera passa d’aquesta pràctica consisteix en la càrrega de les dades. Per simplificar aquest procés, utilitzarem un ``DataLoader`` per defecte de PyTorch. Aquest DataLoader us permetrà carregar les imatges de forma eficient, dividir-les en lots (batches) i aplicar transformacions bàsiques abans de passar-les al model. Això us estalviarà temps i us permetrà centrar-vos en altres aspectes del desenvolupament del model, com la seva construcció i entrenament. A més, el DataLoader gestionarà automàticament la barreja (shuffling) de les dades, la qual cosa és molt important per assegurar un entrenament correcte.\n",
    "\n",
    "\n",
    "Per aquesta pràctica, utilitzarem el conjunt de dades MNIST, que és un dels datasets més coneguts i utilitzats en el camp del machine learning. Aquest conjunt de dades conté imatges en escala de grisos de 28x28 píxels, cadascuna corresponent a un dígit escrit a mà, de l'0 al 9.\n",
    "\n",
    "En aquesta pràctica específica, us centrareu en un problema de classificació binària. Això vol dir que triareu un nombre concret (per exemple, el dígit \"3\") i l'objectiu serà entrenar un model que sigui capaç de diferenciar entre aquest nombre i la resta de dígits. Així, el model haurà de predir si una imatge correspon al dígit seleccionat o si pertany a qualsevol altre nombre. Aquest enfocament us permetrà simplificar el problema inicialment i centrar-vos en comprendre el funcionament dels models de classificació aplicats a imatges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd55e21-5ad3-4f9f-8abc-cd66ad6477f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD = False\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020d402f-1e20-46c8-b4f1-d19d58508ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)), # mitjana, desviacio tipica\n",
    "])\n",
    "\n",
    "\n",
    "train = datasets.MNIST(\"../data\", train=True, download=DOWNLOAD, transform=transform)\n",
    "test = datasets.MNIST(\"../data\", train=False, download=DOWNLOAD, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, BATCH_SIZE)\n",
    "test_loader = torch.utils.data.DataLoader(test, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882943a5-b7b1-4ae1-a6ed-86b8687fe08a",
   "metadata": {},
   "source": [
    "Una bona pràctica es conèixer com són les dades que llegirem. Per fer-ho podem fer ús de la funció shape i les visualitzacions pròpies de ``Matplotlib``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4037df-4f89-4cdf-99fb-261cab2bf0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target = next(iter(train_loader))\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba6656-c0ba-4d1c-84e9-0759e51fc2bb",
   "metadata": {},
   "source": [
    "Quan carreguem les imatges amb el DataLoader de PyTorch, observareu que la seva forma (shape) es representa amb quatre dimensions, habitualment en el format: (batch_size, channels, height, width).\n",
    "\n",
    "Aquestes dimensions són necessàries per tal de poder processar les imatges correctament en lots i gestionar-ne el format. A continuació, es detallen aquestes quatre dimensions:\n",
    "\n",
    "- ``batch_size``: Aquesta dimensió indica el nombre d'imatges que es processen a la vegada en un sol lot (batch). Treballar amb lots permet que el model es pugui entrenar més ràpidament i de forma més estable, ja que es calculen les pèrdues (loss) i les actualitzacions de paràmetres per cada lot en lloc de fer-ho per cada imatge de forma individual.\n",
    "\n",
    "- ``channels``: Aquest valor representa el nombre de canals de color de les imatges. En el cas del conjunt de dades MNIST, les imatges són en escala de grisos, de manera que només tenen un sol canal (channels=1). Si treballéssim amb imatges a color, com les RGB, aquest valor seria 3 (corresponent als canals de vermell, verd i blau).\n",
    "\n",
    "- ``height`` i ``width``: Aquestes dimensions representen l'alçada i l'amplada de cada imatge. Com que les imatges del conjunt de dades MNIST tenen una mida de 28x28 píxels, els valors de height i width seran tots dos iguals a 28.\n",
    "\n",
    "Així, la forma d’una mostra d'imatges MNIST carregada amb el DataLoader podria ser, per exemple, (64, 1, 28, 28), on 64 és la mida del lot (batch_size), 1 és el nombre de canals (escala de grisos), i 28x28 és la mida de cada imatge. Aquesta estructura és fonamental perquè els models de xarxes neuronals convolucionals (CNN) puguin processar les imatges de manera adequada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1167a57-1ce4-41da-a7d6-b93ccadaaddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img[0, 0])\n",
    "plt.title(target[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480261fc-c17d-4399-a605-c4445ca70299",
   "metadata": {},
   "source": [
    "## Definició de la xarxa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d88732-f7f3-4b6f-91d5-722bba6197ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            torch.nn.Linear(784, 10),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.Linear(10, 10),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.Linear(10, 1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1449cc1d-c20e-49cc-8d6d-35cdd303a808",
   "metadata": {},
   "source": [
    "## Entrenament\n",
    "\n",
    "L'entrenament es duu a terme en forma de bucle, i el nombre de vegades que cal repetir aquest bucle s'anomena ``epochs``, un hiperparàmetre que vosaltres haureu de decidir. Cada epoch implica que el model ha vist totes les dades de l'entrenament una vegada, però aquestes es processen en lots més petits, anomenats ``batches``. Això permet que el model pugui entrenar-se de manera més eficient i amb menys ús de memòria.\n",
    "\n",
    "A cada iteració de l'entrenament (és a dir, per cada ``epoch``), la xarxa fa una predicció sobre tots els ``batches`` de dades disponible. Per cada ``batch``, es calcula l'error mitjà de totes les seves mostres d'aquest batch utilitzant una funció de pèrdua. Aquest error serveix per ajustar els pesos de la xarxa de manera que les futures prediccions siguin més precises. Aquest procés es repeteix per tots els ``batches`` i durant el nombre d’``epochs`` que hagueu definit, amb l'objectiu de millorar els resultats finals del model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5815e8-6cca-4c80-9421-a8d706c9d553",
   "metadata": {},
   "source": [
    "Com que el problema que estem abordant és de classificació binària, utilitzarem la funció de pèrdua [``BCEWithLogitsLoss``](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) de PyTorch. Aquesta funció és especialment adequada per a aquest tipus de tasques, ja que combina dues operacions en una de sola: la binary cross entropy (BCE) i l'aplicació de la funció sigmoide.\n",
    "\n",
    "- Funció sigmoide: La funció sigmoide és una funció d'activació que transforma les prediccions de la xarxa en valors entre 0 i 1, el que és ideal per a la classificació binària, ja que podem interpretar aquestes sortides com probabilitats. Per exemple, si la sortida és propera a 1, la xarxa és més segura que la mostra pertany a la classe objectiu, mentre que si és propera a 0, indica que probablement no hi pertany.\n",
    "\n",
    "- Binary Cross Entropy (BCE): La binary cross entropy mesura la diferència entre les prediccions de la xarxa (després de l'aplicació de la sigmoide) i les etiquetes reals de les dades. Això ens permet quantificar el grau d'error en cada predicció, assignant un valor de pèrdua més alt com més diferència hi hagi entre la probabilitat predita i la classe real.\n",
    "\n",
    "La funció ``BCEWithLogitsLoss`` integra aquestes dues operacions en un sol pas, la qual cosa és més eficient tant a nivell computacional com numèricament estable. Així, no és necessari aplicar la funció sigmoide per separat abans de calcular la pèrdua. En resum, emprant aquesta funció de pèrdua, podem assegurar que el model aprèn de forma òptima a distingir entre les dues classes del problema de classificació binària.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a171822-327a-44d4-9e90-350661fdec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()    \n",
    "\n",
    "learning_rate =  1e-3 # Hiperparàmetre\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e1aa16-cfa4-4971-9cf8-ee83eeec7c9b",
   "metadata": {},
   "source": [
    "### Bucle d'entrenament\n",
    "\n",
    "Per poder veure com avança l'entranment empram la llibreria [``tqdm``](https://tqdm.github.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0ad668-cf67-4145-9214-faad610cbae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss = []\n",
    "running_acc = []\n",
    "\n",
    "running_test_loss = []\n",
    "running_test_acc = []\n",
    "\n",
    "for t in tqdm(range(EPOCHS), desc=\"Êpoques\"):\n",
    "    batch_loss = 0\n",
    "    batch_acc = 0\n",
    "    \n",
    "    # Iteram els batches.\n",
    "    for i_batch, (x, y) in tqdm(enumerate(train_loader), desc=f\"Batches (Època {t + 1})\"): \n",
    "        model.train() # Posam el model a mode entranament.\n",
    "        \n",
    "        x = x.reshape(x.shape[0], -1) # Aplanam la imatge.\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 1. PREDICCIÓ\n",
    "        # Passat endavant: calcula la y, fa la predicció passant x al model. \n",
    "        \n",
    "        y_pred = model(x)\n",
    "\n",
    "        # 2. CALCUL DE LA PÈRDUA\n",
    "        # Computa la pèrdua: l'error de predicció vs el valor correcte\n",
    "        # Es guarda la pèrdua en un array per futures visualitzacions\n",
    "\n",
    "        y_binary = (y == 5)\n",
    "        y_binary = y_binary.double()\n",
    "\n",
    "        y_binary = y_binary.reshape(-1, 1)\n",
    "\n",
    "        loss = loss_fn(y_pred, y_binary)\n",
    "\n",
    "        #3. GRADIENT\n",
    "        # Posa a 0 els gradients del model per poder fer la passa enrere\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Passa enrere: calcula el gradient de la pèrdua respecte a tots els paràmetres del model. \n",
    "        # Internament, s'emmagatzemen els paràmetres de cada mòdul a Tensors amb el paràmetere requires_grad=True, d\n",
    "        # de manera que aquesta crida calcularà gradients per tots els paràmetres del model.\n",
    "        loss.backward()\n",
    "\n",
    "        # Actualitza els pesos utilitzant l'algorisme d'actualització\n",
    "        #4. OPTIMITZACIO\n",
    "        with torch.no_grad():\n",
    "            optimizer.step()\n",
    "\n",
    "    \n",
    "        # 5. EVALUAM EL MODEL\n",
    "        model.eval() # Mode avaluació de la xarxa\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        y_pred_binary = (y_pred > 0.5).double()\n",
    "\n",
    "        batch_loss += (loss_fn(y_pred, y_binary).detach())\n",
    "        batch_acc += accuracy_score(y_pred_binary.detach(), y_binary.detach())\n",
    "\n",
    "    running_loss.append(batch_loss / (i_batch + 1))\n",
    "    running_acc.append(batch_acc / (i_batch + 1))\n",
    "\n",
    "\n",
    "    #TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbb4926-dd81-4ebe-ae76-07e6b7175781",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.title(\"BCE per iteració\")\n",
    "plt.plot(running_loss, label=\"train\")\n",
    "plt.plot(running_test_loss, label=\"test\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Accuracy per iteració\")\n",
    "plt.plot(running_acc, label=\"train\")\n",
    "plt.plot(running_test_acc, label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7774ad2-607c-4c4f-bbf8-3c53496ec42f",
   "metadata": {},
   "source": [
    "# Exercicis\n",
    "\n",
    "1. Programa la avaluació del conjunt de test.\n",
    "2. Prova noves configuracions de la xarxa afegint capes ocultes al model. Quina és la configuració que dona la millor classificació en el conjunt de test? i en el de test?\n",
    "3. Hi ha ``overfitting``?\n",
    "4. Prova diferents tasques: classifica diferents nombres. 1 vs la resta, 3 vs la resta.\n",
    "5. Compara els diferents resultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6592bf-ab05-408d-ba7a-505ced994c87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
